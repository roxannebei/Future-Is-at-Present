{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfGwFSYmbl4S"
      },
      "source": [
        "<h2> <b>Set up </b></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz0YK55R00kw"
      },
      "source": [
        "# laod data from Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlgBU2UM1l-M"
      },
      "source": [
        "# check the GPU info\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-5OmmWE1pI5"
      },
      "source": [
        "# import packages\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import keras\n",
        "from keras.layers import Input, Dense, Conv2D, Reshape, BatchNormalization, LeakyReLU, Conv2DTranspose\n",
        "from keras.layers import Flatten, Reshape, LSTM\n",
        "from keras.models import Model, Sequential\n",
        "from keras.datasets import mnist\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras import backend as K\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import cv2\n",
        "import skimage\n",
        "import imutils\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "from imutils import face_utils\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import dlib\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcQZDA0Xb9UM"
      },
      "source": [
        "<h1> <b>Load data from video (.mp4 -> .npy)</b></h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvpt_vXs12oh"
      },
      "source": [
        "# function for save the frames in the .mp4 file into .npy file\n",
        "\n",
        "def load_data_from_video(video_file, colored=False, resize_factor=10, save_npy=True):\n",
        "\n",
        "    ''' --------------- parameter explanation ---------------\n",
        "    video_file: \n",
        "      video file name\n",
        "    colored: \n",
        "      False if the frames in the .npy file are expected to be in grayscale; Ture if the frames in the .npy file are expected to be colored\n",
        "    resize_factor: \n",
        "      the width and height of the frames in the .npy file is how many times smaller than the width and height of the frames in the .mp4 file\n",
        "    ---------------------------------------------------------''' \n",
        "\n",
        "    cap = cv2.VideoCapture(video_file + \".mp4\")\n",
        "    num_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "    print(\"the total number of frames is {}\".format(num_frames))\n",
        "\n",
        "    frame_count = 1\n",
        "\n",
        "    ret, frame0 = cap.read()\n",
        "\n",
        "    # convert the picture to gray or BGR\n",
        "    if colored:\n",
        "        frame0 = cv2.cvtColor(frame0, cv2.COLOR_BGR2RGB)\n",
        "    else:\n",
        "        frame0 = cv2.cvtColor(frame0, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # resize the picture for lighter training\n",
        "    if resize_factor != 1:\n",
        "        frame0 = imutils.resize(frame0, width=int(1280/resize_factor))\n",
        "\n",
        "    # reshape the picture\n",
        "    if colored:\n",
        "        frame0 = np.reshape(frame0, (1, frame0.shape[0], frame0.shape[1], 3))\n",
        "    else:\n",
        "        frame0 = np.reshape(frame0, (1, frame0.shape[0], frame0.shape[1], 1))\n",
        "\n",
        "    data = frame0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if colored:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # resize the picture for lighter training\n",
        "        if resize_factor != 1:\n",
        "            frame = imutils.resize(frame, width=int(1280/resize_factor))\n",
        "\n",
        "        if colored:\n",
        "            frame = np.reshape(frame, (1, frame.shape[0], frame.shape[1], 3))\n",
        "        else:\n",
        "            frame = np.reshape(frame, (1, frame.shape[0], frame.shape[1], 1))\n",
        "\n",
        "        data = np.concatenate((data, frame), axis=0)\n",
        "\n",
        "        frame_count += 1\n",
        "        if frame_count % 100 == 0:\n",
        "            print(\"the current loading frame is {}\".format(frame_count))\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    print(data.shape)\n",
        "\n",
        "    if save_npy:\n",
        "        if colored:\n",
        "            np.save(video_file + \"_colored_data.npy\", data)\n",
        "        else:\n",
        "            np.save(video_file + \"gray_data.npy\", data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f53ueg3ccWNz"
      },
      "source": [
        "video_file = \"human_aging\"\n",
        "colored = False\n",
        "\n",
        "# uncomment if the video is not loaded before\n",
        "# load_data_from_video(video_file, colored=colored, resize_factor=2, save_npy=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44ouf7IncXCk"
      },
      "source": [
        "# load the .npy file from Google drive\n",
        "data = np.load(\"/content/drive/MyDrive/human_aging_gray_data_large.npy\")\n",
        "    \n",
        "data = np.reshape(data, (7428, 360, 640))\n",
        "\n",
        "frame_num = data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYq6DlvBdhla"
      },
      "source": [
        "#display some examples\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.imshow(data[2000], cmap=plt.cm.gray)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.imshow(data[7000], cmap=plt.cm.gray)\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpJltDPyd0VX"
      },
      "source": [
        "<h1> <b>Crop the frames </b></h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0mMEoEMdvvI"
      },
      "source": [
        "''' --------------- cropping method 1 ---------------\n",
        "    Goal: crop the image to reduce computational cost while\n",
        "      1. keep the entire face as much as possible (including hair)\n",
        "      2. keep the information in height changes and make the growing up trend visible\n",
        "    Parameter explanation:\n",
        "      1. the resize_factor is the same resize_factor to what in the load data from video function above\n",
        "      2. center_x & center_y: center_x and center_y are gained by input the image into the get_center function below\n",
        "    ---------------------------------------------------------''' \n",
        "\n",
        "def crop(image, resize_factor, center_x, center_y):\n",
        "    h = int(70 * 10/resize_factor)\n",
        "    w = int(55 * 10/resize_factor)\n",
        "    x = int(center_x - w/2)\n",
        "    y = 0\n",
        "    #box = (x, y, w, h)\n",
        "    roi = image[y:y+h, x:x+w]\n",
        "    #roi = image.crop(box)\n",
        "    return roi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSQVr0iXl-sH"
      },
      "source": [
        "''' --------------- cropping method 2 ---------------\n",
        "    Goal: crop the image to reduce computational cost while\n",
        "      1. make the position of the center of the face to a fixed point in each frame\n",
        "    Parameter explanation:\n",
        "      1. resize_factor: the resize_factor is the same resize_factor to what in the load data from video function above\n",
        "      2. center_x & center_y: center_x and center_y are gained by input the image into the get_center function below\n",
        "    ---------------------------------------------------------''' \n",
        "\n",
        "def crop(image, resize_factor, center_x, center_y):\n",
        "  w = int(55 * 10/resize_factor)\n",
        "  roi = image[int(center_y-34 * 10/resize_factor):int(center_y+22 * 10/resize_factor), int(center_x-w/2):int(center_x+w/2)]\n",
        "  return roi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHjekncQoOtp"
      },
      "source": [
        "# calculate the center of the face with facial landmarks\n",
        "# dlib open source with: shape_predictor_68_face_landmarks.dat\n",
        "\n",
        "\n",
        "def get_center(image):\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor(\"/content/drive/MyDrive/shape_predictor_68_face_landmarks.dat\")\n",
        "    #gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    gray = image\n",
        "    rects = detector(gray, 1)\n",
        "    X = []\n",
        "    Y = []\n",
        "    for (i, rect) in enumerate(rects):\n",
        "        shape = predictor(gray, rect)\n",
        "        shape = face_utils.shape_to_np(shape)\n",
        "        for (name, (i, j)) in face_utils.FACIAL_LANDMARKS_IDXS.items():\n",
        "            for (x, y) in shape[i:j]:\n",
        "#                 cv2.circle(image, (x, y), 1, (255, 0, 0), -1)\n",
        "#                 cv2.circle(gray, (x, y), 2, (0, 0, 255), -1)\n",
        "            #print(\"x: \" + str(x) + \", y: \" + str(y))\n",
        "                X.append(x)\n",
        "                Y.append(y)\n",
        "        #cv2.circle(image, (int(np.mean(X)), int(np.mean(Y))), 1, (0, 255, 0), -1)\n",
        "        center_x = int(np.mean(X))\n",
        "        center_y = int(np.mean(Y))\n",
        "        #cv2.circle(image, center_x, center_y, 1, (0, 255, 0), -1)\n",
        "        #output = face_utils.visualize_facial_landmarks(image, shape)\n",
        "        #image = crop1(image, 10, center_x, center_y)\n",
        "        # print(\"center_y: \" + str(center_y))\n",
        "        # print(\"max(Y): \" + str(max(Y)))\n",
        "        # print(\"max(Y)-center_y: \" + str(max(Y)-center_y))\n",
        "        range_X = int(np.max(X))-int(np.min(X))\n",
        "        #print(\"range X: \" + str(range_X) + \"\\n\")\n",
        "    #plt.figure(figsize=(10,6))\n",
        "    #plt.imshow(image)\n",
        "    return center_x, center_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzRzysuJm1UU"
      },
      "source": [
        "# plot some sample frames after cropping to test if the parameters in the cropping methods are correct  \n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "a, b =2, 4\n",
        "for i in range(8):  \n",
        "    plt.subplot(a, b, i+1)\n",
        "    image = data[1000*i]\n",
        "    x, y = get_center(image) \n",
        "    roi = crop(image, 2, x, y) # works for either cropping method 1 or cropping method 2\n",
        "    plt.imshow(roi, cmap=plt.cm.gray)\n",
        "    plt.axis('off')\n",
        "    plt.title('frame: ' + str(1000*i))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtjqkFc2m8rw"
      },
      "source": [
        "# crop the frames to reduce the computational cost and save them as .npy file to Google drive\n",
        "crop_data = []\n",
        "for i in range(data.shape[0]):\n",
        "  image = data[i]\n",
        "  x, y = get_center(image)\n",
        "  roi = crop(image, 2, x, y)\n",
        "  crop_data.append(roi)\n",
        "\n",
        "#np.save(\"/content/drive/MyDrive/cropped_data_large_fixed.npy\",crop_data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}